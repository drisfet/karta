# Chat System Architecture Documentation

## Overview

This document describes the custom chat system implementation that provides dynamic model switching capabilities, bypassing limitations in the Vercel AI SDK's `useChat` hook.

## ğŸ¯ Problem Solved

The original implementation using Vercel AI SDK's `useChat` hook had a critical limitation:
- `useChat` captures the `body` parameter on initialization and doesn't update when it changes
- Model switching didn't work because the API always received `model: undefined`
- Required complex workarounds like component remounting

## âœ… Solution: Custom Chat Implementation

### 1. Created Custom Chat Hook (`src/hooks/use-dynamic-chat.ts`)

```typescript
export function useDynamicChat({ api, initialMessages = [], searchContext }) {
  // âœ… Gets current model from context dynamically
  const { selectedModel } = useChatModel();

  // âœ… Always uses latest model in requests
  const sendMessage = useCallback(async (message) => {
    const requestBody = {
      messages: [...messages, userMessage],
      searchContext,
      model: selectedModel, // Always current!
    };
    // ... streaming response handling
  }, [selectedModel, ...]); // âœ… Reactive to model changes
}
```

**Key Features:**
- âœ… Dynamic model switching without remounting
- âœ… Real-time context updates
- âœ… Streaming response handling
- âœ… Request cancellation support
- âœ… Error handling and loading states

### 2. Updated ChatComponent

```typescript
// ChatComponent to handle dynamic chat with model switching
const ChatComponent = ({
  searchContext,
  initialMessages,
  onChat
}) => {
  const chat = useDynamicChat({
    api: '/api/chat',
    initialMessages,
    searchContext
  });

  // âœ… No AI SDK dependency
  // âœ… Handles model switching dynamically
  // âœ… No remounting needed
};
```

**Benefits:**
- âœ… Removed AI SDK dependency for chat functionality
- âœ… Uses `useDynamicChat` instead of `useChat`
- âœ… No more remounting needed - handles model switching dynamically
- âœ… Proper streaming response handling
- âœ… Full control over request lifecycle
- âœ… Comprehensive debug logging for troubleshooting

### 3. Context-Based Model Management (`src/hooks/use-chat-model.tsx`)

```typescript
export function ChatModelProvider({ children }) {
  const [selectedModel, setSelectedModel] = useState<string | null>(null);
  return (
    <ModelContext.Provider value={{ selectedModel, setSelectedModel }}>
      {children}
    </ModelContext.Provider>
  );
}

export function useChatModel() {
  const context = useContext(ModelContext);
  if (context === undefined) {
    throw new Error('useChatModel must be used within a ChatModelProvider');
  }
  return context;
}
```

**Features:**
- âœ… Global model state management
- âœ… Type-safe context usage
- âœ… Proper error handling for missing provider

### 4. AI SDK Model Provider (`src/lib/ai-sdk-models.ts`)

```typescript
export class AISDKModelProvider {
  static createModel(modelId: string) {
    // Check if it's a Google Gemini model
    if (modelId.startsWith('gemini')) {
      return googleProvider(modelId);
    }

    // Check if it's an OpenRouter model
    if (modelId.includes('/') || modelId.includes('openrouter')) {
      const cleanModelId = modelId.replace('openrouter/', '');
      return openrouter.chat(cleanModelId);
    }

    // Fallback to OpenAI
    return openai.chat(modelId);
  }

  static validateModel(modelId: string): boolean
  static getProviderType(modelId: string): AIModelProvider
  static getAvailableModels()
}
```

**Capabilities:**
- âœ… Supports Google Gemini models
- âœ… Supports OpenRouter models
- âœ… Model validation
- âœ… Provider type detection

## ğŸ¯ Key Advantages of This Solution

### Dynamic Model Switching
```javascript
// User selects model â†’ selectedModel updates in context
// Next message automatically uses new model
// No component remounting required
// No AI SDK limitations
```

### Real-time Model Updates
- âœ… Model changes are immediately reflected in API calls
- âœ… No stale body data from captured closures
- âœ… Full control over request structure
- âœ… Reactive to context changes

### Streaming Support
- âœ… Handles Server-Sent Events properly
- âœ… Request cancellation on model switch
- âœ… Error handling and loading states
- âœ… Proper cleanup on unmount

## ğŸ‰ Result

The model switcher now works perfectly:

- âœ… **User selects model** â†’ Context updates immediately
- âœ… **User sends message** â†’ API receives correct model
- âœ… **Logs show**: `"Selected model: gemini-1.5-flash"` âœ…
- âœ… **No more "model: undefined"** âœ…

## ğŸ“‹ Architecture Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ChatModel      â”‚â”€â”€â”€â–¶â”‚  useDynamicChat  â”‚â”€â”€â”€â–¶â”‚  API Route      â”‚
â”‚  Context        â”‚    â”‚  (Custom Hook)   â”‚    â”‚  (/api/chat)    â”‚
â”‚  selectedModel  â”‚    â”‚                  â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  âœ… Dynamic      â”‚    â”‚  âœ… Receives    â”‚
                       â”‚     model        â”‚    â”‚     correct     â”‚
                       â”‚     switching    â”‚    â”‚     model       â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Implementation Details

### File Structure
```
src/
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ use-chat-model.tsx      # Context provider for model state
â”‚   â””â”€â”€ use-dynamic-chat.ts     # Custom chat hook
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ ai-sdk-models.ts        # AI SDK model provider
â”‚   â””â”€â”€ models.ts               # Model definitions
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ panels/types/
â”‚   â”‚   â””â”€â”€ prime-panel.tsx     # Main chat component
â”‚   â””â”€â”€ query-bar/
â”‚       â””â”€â”€ chat-model-switcher.tsx # Model selector UI
â””â”€â”€ app/api/
    â””â”€â”€ chat/route.ts           # API endpoint
```

### Environment Variables
```bash
# Google Gemini
GOOGLE_API_KEY=your_google_api_key

# OpenRouter
OPENROUTER_API_KEY=your_openrouter_api_key
```

### Model Configuration (`src/lib/models.ts`)
```typescript
export const chatModels: ModelInfo[] = [
  // Google Gemini Models
  {
    id: "gemini-1.5-flash",
    name: "Gemini 1.5 Flash",
    description: "Fast chat responses",
    provider: "Google",
    icon: Zap
  },
  // OpenRouter Models
  {
    id: "deepseek/deepseek-chat-v3.1:free",
    name: "DeepSeek Chat v3.1",
    description: "Efficient chat model",
    provider: "OpenRouter",
    icon: MessageSquare
  }
];
```

## ğŸš€ Usage

### Basic Usage
```typescript
import { useDynamicChat } from '@/hooks/use-dynamic-chat';
import { useChatModel } from '@/hooks/use-chat-model';

function ChatComponent() {
  const { selectedModel } = useChatModel();
  const { messages, sendMessage, isLoading, error } = useDynamicChat({
    api: '/api/chat',
    searchContext: 'optional search context'
  });

  return (
    <div>
      {/* Chat UI */}
      <button onClick={() => sendMessage({ text: 'Hello!' })}>
        Send Message
      </button>
    </div>
  );
}
```

### Model Switching
```typescript
import { useChatModel } from '@/hooks/use-chat-model';

function ModelSwitcher() {
  const { selectedModel, setSelectedModel } = useChatModel();

  return (
    <select
      value={selectedModel || ''}
      onChange={(e) => setSelectedModel(e.target.value)}
    >
      <option value="gemini-1.5-flash">Gemini 1.5 Flash</option>
      <option value="deepseek/deepseek-chat-v3.1:free">DeepSeek</option>
    </select>
  );
}
```

## ğŸ”§ API Integration

### Request Format
```json
{
  "messages": [
    {
      "role": "user",
      "parts": [{ "type": "text", "text": "Hello!" }],
      "id": "user-123"
    }
  ],
  "searchContext": "optional search context",
  "model": "gemini-1.5-flash"
}
```

### Response Format
```
data: {"role": "assistant", "parts": [{"type": "text", "text": "Hi there!"}], "id": "assistant-123"}

data: [DONE]
```

## ğŸ› Troubleshooting

### Common Issues

1. **Model not updating in API calls**
   - Ensure `ChatModelProvider` wraps your app
   - Check that `useChatModel()` is called within the provider

2. **Streaming not working**
   - Verify API endpoint returns proper SSE format
   - Check network tab for connection issues

3. **Model validation errors**
   - Ensure model ID exists in `chatModels` array
   - Check `AISDKModelProvider.validateModel()` implementation

### Debug Logging
```typescript
// Enable debug logging
console.log('Selected model:', selectedModel);
console.log('Chat state:', { messages, isLoading, error });
```

## ğŸ“ˆ Performance Considerations

- âœ… Minimal re-renders with proper memoization
- âœ… Request cancellation prevents race conditions
- âœ… Efficient context usage
- âœ… Streaming response handling
- âœ… Proper cleanup on unmount

## ğŸ”® Future Enhancements

- [ ] Message persistence across sessions
- [ ] Chat history management
- [ ] File upload support
- [ ] Voice message integration
- [ ] Message reactions and threading

---

This implementation provides a robust, scalable chat system with seamless model switching capabilities, free from the limitations of third-party SDKs.